{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9508924,
          "sourceType": "datasetVersion",
          "datasetId": 5787852
        }
      ],
      "dockerImageVersionId": 30776,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebookd4d99df5a9",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandrNizkovskikh/AlexandrNizkovskikh/blob/main/notebookd4d99df5a9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'tanebaum-ostin:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5787852%2F9508924%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T155423Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D082dd591d848d07fe3756f41fce96da745d0d83d7821fc32ccb5f2119d5a81a504d19c6a9fccd83d2cf2aed50111560cb08589767864d77d93a862937bf5519f781c64c8fd697c030e0385248384ffa468de28a1b9300d4b1407dc4f3d2acdd90090583ca087c5a64d97eaabf98eef6177c474e2ed6ab8da366a8b507d349432d45d25eb447e0d5c9ecfcf595c7dcd8d48b54798f096dd20aca053737c15590d52984b9496f71fce778b5ef03a260998e210ab35ae74fd9945bf90e3ed4cdc91bff8b1eab007ecea157e74a1ce3a00f6da5cb6b5d37935a31d8d476bfde6d586fc165e917d35292b41f933b2ba176b58cb2dc1bf9d60453f59858e0dfc93007e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5k3N6VGro-nR"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение\n",
        "\n",
        "Этот проект представляет собой реализацию системы для обработки запросов с использованием больших языковых моделей (LLM), таких как Saiga Mistral 7b, с интеграцией популярных библиотек для векторных индексов и эмбеддингов, включая Hugging Face, Llama Index и Langchain.\n",
        "Код позволяет эффективно управлять большими текстовыми данными, структурировать и анализировать их, а также выполнять точные и релевантные ответы на запросы пользователей на основе предобученных моделей и векторных представлений."
      ],
      "metadata": {
        "id": "v5SenozYo-nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почему Saiga Mistral 7b?\n",
        "\n",
        "Saiga Mistral 7b — это одна из наиболее продвинутых и сбалансированных моделей для выполнения языковых задач, таких как генерация текста, ответы на вопросы и обработка больших объемов данных.\n",
        "\n",
        "Вот несколько причин, почему Saiga Mistral 7b была выбрана для этого проекта:\n",
        "\n",
        "Компактный размер и мощность: Модель содержит 7 миллиардов параметров, что делает её мощной, но одновременно достаточно компактной, чтобы эффективно работать на современных аппаратных платформах с ограниченными ресурсами (в том числе в режиме квантования).\n",
        "\n",
        "Поддержка квантования: В проекте используется квантование для работы с моделью в 4-битном режиме. Это позволяет существенно снизить нагрузку на GPU и ускорить вычисления без значительных потерь в качестве ответов. Saiga Mistral 7b поддерживает методы, такие как bnb_4bit и nf4, для этого сценария.\n",
        "\n",
        "Качество генерации текста: Модель демонстрирует отличные результаты в задачах генерации связного и осмысленного текста, что важно для систем обработки естественного языка, требующих высокой степени контекстуального понимания.\n",
        "\n",
        "Гибкость дообучения: Модель совместима с методами дообучения, такими как PEFT и LoRA, что делает её легко адаптируемой к специфическим задачам и данным.\n",
        "\n",
        "Русскоязычная поддержка: Saiga Mistral 7b показывает отличные результаты на русскоязычных данных, что критично для проектов, ориентированных на русскоязычную аудиторию."
      ],
      "metadata": {
        "id": "wEezQH82o-nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Установка библиотек"
      ],
      "metadata": {
        "id": "jAufz9Cho-nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install llama_index pyvis Ipython langchain pypdf langchain_community\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentencepiece accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install peft\n",
        "!pip install openai"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-30T15:34:06.004399Z",
          "iopub.execute_input": "2024-09-30T15:34:06.004921Z",
          "iopub.status.idle": "2024-09-30T15:37:13.037037Z",
          "shell.execute_reply.started": "2024-09-30T15:34:06.004887Z",
          "shell.execute_reply": "2024-09-30T15:37:13.035911Z"
        },
        "trusted": true,
        "id": "jkCCiPeYo-nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "vaNvdaMMo-nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:13.039198Z",
          "iopub.execute_input": "2024-09-30T15:37:13.039585Z",
          "iopub.status.idle": "2024-09-30T15:37:13.045066Z",
          "shell.execute_reply.started": "2024-09-30T15:37:13.039547Z",
          "shell.execute_reply": "2024-09-30T15:37:13.043966Z"
        },
        "trusted": true,
        "id": "UF30pgy8o-nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, Document, GPTVectorStoreIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:13.046206Z",
          "iopub.execute_input": "2024-09-30T15:37:13.046518Z",
          "iopub.status.idle": "2024-09-30T15:37:34.105699Z",
          "shell.execute_reply.started": "2024-09-30T15:37:13.046484Z",
          "shell.execute_reply": "2024-09-30T15:37:34.104916Z"
        },
        "trusted": true,
        "id": "zDdq3k9Co-nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Аутентификация HF и OpenAI"
      ],
      "metadata": {
        "id": "p5wDzdBXo-nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "HF_TOKEN = getpass.getpass(\"Вставьте ваш токен: \")\n",
        "\n",
        "# Выполняем аутентификацию\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:37:34.10791Z",
          "iopub.execute_input": "2024-09-30T15:37:34.108531Z",
          "iopub.status.idle": "2024-09-30T15:39:16.453137Z",
          "shell.execute_reply.started": "2024-09-30T15:37:34.108495Z",
          "shell.execute_reply": "2024-09-30T15:39:16.452224Z"
        },
        "trusted": true,
        "id": "iAfYisipo-nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass # для работы с паролями\n",
        "import os      # для работы с окружением и файловой системой\n",
        "\n",
        "# Запрос ввода ключа от OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:16.454214Z",
          "iopub.execute_input": "2024-09-30T15:39:16.454499Z",
          "iopub.status.idle": "2024-09-30T15:39:21.663903Z",
          "shell.execute_reply.started": "2024-09-30T15:39:16.454467Z",
          "shell.execute_reply": "2024-09-30T15:39:21.663146Z"
        },
        "trusted": true,
        "id": "fd6DMLC7o-nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как языковая модель saiga_mistral_7b_lora обучена для ведения диалогов, то для нее определены специальные теги.\n",
        "\n",
        "Сообщения к модели строиться по шаблону:\n",
        "\n",
        "< s >{role}\\n{content}< /s >,\n",
        "\n",
        "где content - это текст сообщения к модели, role - одна из возможных ролей:\n",
        "\n",
        "system - системная роль, определяет преднастройки модели\n",
        "user - вопросы от пользователей"
      ],
      "metadata": {
        "id": "7JOXlzL4o-nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_to_prompt(messages):\n",
        "    prompt = \"\"\n",
        "    for message in messages:\n",
        "        if message.role == 'system':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'user':\n",
        "            prompt += f\"<s>{message.role}\\n{message.content}</s>\\n\"\n",
        "        elif message.role == 'bot':\n",
        "            prompt += f\"<s>bot\\n\"\n",
        "\n",
        "    # ensure we start with a system prompt, insert blank if needed\n",
        "    if not prompt.startswith(\"<s>system\\n\"):\n",
        "        prompt = \"<s>system\\n</s>\\n\" + prompt\n",
        "\n",
        "    # add final assistant prompt\n",
        "    prompt = prompt + \"<s>bot\\n\"\n",
        "    return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<s>system\\n</s>\\n<s>user\\n{completion}</s>\\n<s>bot\\n\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:21.665029Z",
          "iopub.execute_input": "2024-09-30T15:39:21.66542Z",
          "iopub.status.idle": "2024-09-30T15:39:21.675214Z",
          "shell.execute_reply.started": "2024-09-30T15:39:21.665371Z",
          "shell.execute_reply": "2024-09-30T15:39:21.674166Z"
        },
        "trusted": true,
        "id": "9N6izLEzo-nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка модели"
      ],
      "metadata": {
        "id": "Vbu8o68Go-nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализована поддержка дообучения модели через PEFT (Parameter-Efficient Fine-Tuning) с использованием метода LoRA, что позволяет адаптировать модель под специфические задачи с минимальными вычислительными затратами"
      ],
      "metadata": {
        "id": "te7WdJ9do-nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "# Определяем параметры квантования, иначе модель не выполниться в колабе\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Задаем имя модели\n",
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "\n",
        "# Создание конфига, соответствующего методу PEFT (в нашем случае LoRA)\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем базовую модель, ее имя берем из конфига для LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,          # идентификатор модели\n",
        "    quantization_config=quantization_config, # параметры квантования\n",
        "    torch_dtype=torch.float16,               # тип данных\n",
        "    device_map=\"auto\"                        # автоматический выбор типа устройства\n",
        ")\n",
        "\n",
        "# Загружаем LoRA модель\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Переводим модель в режим инференса\n",
        "# Можно не переводить, но явное всегда лучше неявного\n",
        "model.eval()\n",
        "\n",
        "# Загружаем токенизатор\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:39:21.676534Z",
          "iopub.execute_input": "2024-09-30T15:39:21.676923Z",
          "iopub.status.idle": "2024-09-30T15:40:56.014373Z",
          "shell.execute_reply.started": "2024-09-30T15:39:21.676887Z",
          "shell.execute_reply": "2024-09-30T15:40:56.013542Z"
        },
        "trusted": true,
        "id": "MyF97pNGo-nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод конфигурации для модели"
      ],
      "metadata": {
        "id": "3gu7wIDao-na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "print(generation_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.015642Z",
          "iopub.execute_input": "2024-09-30T15:40:56.016626Z",
          "iopub.status.idle": "2024-09-30T15:40:56.191926Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.016586Z",
          "shell.execute_reply": "2024-09-30T15:40:56.190972Z"
        },
        "trusted": true,
        "id": "rveFcgT9o-na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    model=model,             # модель\n",
        "    model_name=MODEL_NAME,   # идентификатор модели\n",
        "    tokenizer=tokenizer,     # токенизатор\n",
        "    max_new_tokens=generation_config.max_new_tokens, # параметр необходимо использовать здесь, и не использовать в generate_kwargs, иначе ошибка двойного использования\n",
        "    model_kwargs={\"quantization_config\": quantization_config}, # параметры квантования\n",
        "    generate_kwargs = {   # параметры для инференса\n",
        "      \"bos_token_id\": generation_config.bos_token_id, # токен начала последовательности\n",
        "      \"eos_token_id\": generation_config.eos_token_id, # токен окончания последовательности\n",
        "      \"pad_token_id\": generation_config.pad_token_id, # токен пакетной обработки (указывает, что последовательность ещё не завершена)\n",
        "      \"no_repeat_ngram_size\": generation_config.no_repeat_ngram_size,\n",
        "      \"repetition_penalty\": generation_config.repetition_penalty,\n",
        "      \"temperature\": generation_config.temperature,\n",
        "      \"do_sample\": True,\n",
        "      \"top_k\": 50,\n",
        "      \"top_p\": 0.95\n",
        "    },\n",
        "    messages_to_prompt=messages_to_prompt,     # функция для преобразования сообщений к внутреннему формату\n",
        "    completion_to_prompt=completion_to_prompt, # функции для генерации текста\n",
        "    device_map=\"auto\",                         # автоматически определять устройство\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.193063Z",
          "iopub.execute_input": "2024-09-30T15:40:56.193354Z",
          "iopub.status.idle": "2024-09-30T15:40:56.626701Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.193323Z",
          "shell.execute_reply": "2024-09-30T15:40:56.625798Z"
        },
        "trusted": true,
        "id": "HrL1IxLho-na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Работа с базой данных"
      ],
      "metadata": {
        "id": "MDAOK-Uto-na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация данных производится с использованием многозадачных эмбеддингов sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2, что обеспечивает поддержку нескольких языков, включая русский"
      ],
      "metadata": {
        "id": "1B1fugvDo-na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface  import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:40:56.631083Z",
          "iopub.execute_input": "2024-09-30T15:40:56.631843Z",
          "iopub.status.idle": "2024-09-30T15:41:02.715195Z",
          "shell.execute_reply.started": "2024-09-30T15:40:56.631803Z",
          "shell.execute_reply": "2024-09-30T15:41:02.714236Z"
        },
        "trusted": true,
        "id": "9ZQ7Q0-5o-na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка ServiceContext (глобальная настройка параметров LLM)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:41:02.716466Z",
          "iopub.execute_input": "2024-09-30T15:41:02.717237Z",
          "iopub.status.idle": "2024-09-30T15:41:03.722Z",
          "shell.execute_reply.started": "2024-09-30T15:41:02.7172Z",
          "shell.execute_reply": "2024-09-30T15:41:03.721145Z"
        },
        "trusted": true,
        "id": "ZBKaOgfoo-na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MOhmeob4o-nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка текстов и структурирования данных, включая возможность извлечения табличной информации из текстовых документов"
      ],
      "metadata": {
        "id": "uKUjUy4Go-nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def extract_structured_data(text):\n",
        "\n",
        "    tables = []\n",
        "    table_pattern = re.compile(r'(\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+(?:\\.\\d+)?(?:\\s+|,|\\t)\\d+)')\n",
        "\n",
        "    for match in table_pattern.finditer(text):\n",
        "        table_data = match.group(0)\n",
        "        # Разделение строки на столбцы по пробелам или запятым\n",
        "        columns = re.split(r'\\s+|,|\\t', table_data)\n",
        "        tables.append(columns)\n",
        "\n",
        "    if tables:\n",
        "        # Преобразуем в DataFrame для структурированного хранения\n",
        "        df = pd.DataFrame(tables)\n",
        "        return df\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def preprocess_document(doc):\n",
        "    if len(doc.text) < 100:  # Фильтрация по длине документа\n",
        "        return None\n",
        "\n",
        "    # Извлекаем структурированные данные из текста\n",
        "    structured_data = extract_structured_data(doc.text)\n",
        "\n",
        "    # Если данные представлены в виде DataFrame, преобразуем их в список\n",
        "    if isinstance(structured_data, pd.DataFrame):\n",
        "        structured_data = structured_data.to_dict(orient=\"list\")  # Преобразуем в сериализуемый формат\n",
        "\n",
        "    # Возвращаем объект Document с текстом и метаданными\n",
        "    return Document(text=doc.text, metadata={\"structured_data\": structured_data})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:41:03.723317Z",
          "iopub.execute_input": "2024-09-30T15:41:03.723729Z",
          "iopub.status.idle": "2024-09-30T15:41:03.732952Z",
          "shell.execute_reply.started": "2024-09-30T15:41:03.723675Z",
          "shell.execute_reply": "2024-09-30T15:41:03.731979Z"
        },
        "trusted": true,
        "id": "5ygCdQnjo-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для примера работы загружается книга \"Архитектура компьютера\""
      ],
      "metadata": {
        "id": "fIsvHPJQo-nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка и предобработка документов\n",
        "documents = [preprocess_document(doc) for doc in SimpleDirectoryReader('/kaggle/input/tanebaum-ostin').load_data() if doc]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:43:04.773274Z",
          "iopub.execute_input": "2024-09-30T15:43:04.773727Z",
          "iopub.status.idle": "2024-09-30T15:44:53.89585Z",
          "shell.execute_reply.started": "2024-09-30T15:43:04.773686Z",
          "shell.execute_reply": "2024-09-30T15:44:53.895005Z"
        },
        "trusted": true,
        "id": "D0LW9zM1o-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Фильтрация None значений после предобработки\n",
        "documents = [doc for doc in documents if doc is not None]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:26.815693Z",
          "iopub.execute_input": "2024-09-30T15:45:26.816615Z",
          "iopub.status.idle": "2024-09-30T15:45:26.821271Z",
          "shell.execute_reply.started": "2024-09-30T15:45:26.816573Z",
          "shell.execute_reply": "2024-09-30T15:45:26.820334Z"
        },
        "trusted": true,
        "id": "-uDjXfe9o-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Система использует GPTVectorStoreIndex для создания векторных представлений документов, что позволяет эффективно искать и извлекать информацию на основе сходства текстов"
      ],
      "metadata": {
        "id": "G2M1q4klo-nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = GPTVectorStoreIndex.from_documents(\n",
        "\tdocuments\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:29.639815Z",
          "iopub.execute_input": "2024-09-30T15:45:29.640212Z",
          "iopub.status.idle": "2024-09-30T15:45:43.275195Z",
          "shell.execute_reply.started": "2024-09-30T15:45:29.640176Z",
          "shell.execute_reply": "2024-09-30T15:45:43.274367Z"
        },
        "trusted": true,
        "id": "NyD1fUD6o-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация запроса по длинне символов"
      ],
      "metadata": {
        "id": "CENho9D2o-nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_query(query, min_length=10, max_length=100):\n",
        "    \"\"\"\n",
        "    Функция классифицирует запрос на короткий, длинный или пустой.\n",
        "\n",
        "    Аргументы:\n",
        "    - query: строка запроса.\n",
        "    - min_length: минимальная длина для \"корректного\" запроса.\n",
        "    - max_length: максимальная длина для \"корректного\" запроса.\n",
        "\n",
        "    Возвращает:\n",
        "    - строка с результатом классификации.\n",
        "    \"\"\"\n",
        "    # Удаляем лишние пробелы\n",
        "    query = query.strip()\n",
        "\n",
        "    # Проверка на пустой запрос\n",
        "    if not query:\n",
        "        return \"Запрос пуст.\"\n",
        "\n",
        "    # Проверка на короткий запрос\n",
        "    if len(query) < min_length:\n",
        "        return f\"Запрос слишком короткий. Длина запроса: {len(query)} символов.\"\n",
        "\n",
        "    # Проверка на длинный запрос\n",
        "    if len(query) > max_length:\n",
        "        return f\"Запрос слишком длинный. Длина запроса: {len(query)} символов.\"\n",
        "\n",
        "    return \"Запрос корректный.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:53.034091Z",
          "iopub.execute_input": "2024-09-30T15:45:53.034967Z",
          "iopub.status.idle": "2024-09-30T15:45:53.041584Z",
          "shell.execute_reply.started": "2024-09-30T15:45:53.034925Z",
          "shell.execute_reply": "2024-09-30T15:45:53.040456Z"
        },
        "trusted": true,
        "id": "en7coH0Ao-nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка работы модели"
      ],
      "metadata": {
        "id": "pyP00ngOo-nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример запроса"
      ],
      "metadata": {
        "id": "ap7-vMeUo-nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Серверы работают под управлением каких операционных систем? Поддерживаются ли UNIX и Windows?\"\n",
        "classify_query(query)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:45:56.527099Z",
          "iopub.execute_input": "2024-09-30T15:45:56.527836Z",
          "iopub.status.idle": "2024-09-30T15:45:56.534535Z",
          "shell.execute_reply.started": "2024-09-30T15:45:56.52777Z",
          "shell.execute_reply": "2024-09-30T15:45:56.533611Z"
        },
        "trusted": true,
        "id": "Roo3aN6wo-nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример ответа"
      ],
      "metadata": {
        "id": "EAz7ZQo-o-nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10\n",
        ")\n",
        "\n",
        "message_template = f\"\"\"<s>system\n",
        "Ты являешься моделью, которая отвечает только на основании предоставленных источников.\n",
        "Отвечай строго на основе информации из текста.\n",
        "Если нужной информации нет в источнике, ответь: 'я не знаю'. Не добавляй ничего, что не указано в тексте. Не придумывай и не добавляй лишние данные.\n",
        "\n",
        "<s>user\n",
        "Вопрос: {query}\n",
        "Источник:\n",
        "</s>\n",
        "\"\"\"\n",
        "#\n",
        "response = query_engine.query(message_template)\n",
        "#\n",
        "print()\n",
        "print('Ответ:')\n",
        "print(response.response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-30T15:46:00.321436Z",
          "iopub.execute_input": "2024-09-30T15:46:00.32234Z",
          "iopub.status.idle": "2024-09-30T15:46:50.176871Z",
          "shell.execute_reply.started": "2024-09-30T15:46:00.322298Z",
          "shell.execute_reply": "2024-09-30T15:46:50.175769Z"
        },
        "trusted": true,
        "id": "1FmwRij3o-nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}